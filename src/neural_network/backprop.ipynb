{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(42) # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the dataset\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "block_size = 3  # context size: how many characters we consider to predict the next character\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # update context with the next character\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # training set\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # validation set\n",
    "Xte, Yte = build_dataset(words[n2:]) # test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to compare manual gradients with pytorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item() # exact match\n",
    "    app = torch.allclose(dt, t.grad) # approximate match\n",
    "    maxdiff = (dt - t.grad).abs().max().item() # max difference\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff={maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4137\n"
     ]
    }
   ],
   "source": [
    "# initialize the parameters\n",
    "n_emb = 10 # embedding size\n",
    "n_hidden = 64 # hidden layer size\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((vocab_size, n_emb), generator=g) # embedding matrix\n",
    "W1 = torch.randn((n_emb * block_size, n_hidden), generator=g) * (5/3) / ((n_emb * block_size) ** 0.5) # first layer initialized with small values, scaled down\n",
    "b1 = torch.randn((n_hidden), generator=g) * 0.1 # bias for the first layer\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1 # second layer, initialized with small values, scaled down\n",
    "b2 = torch.randn((vocab_size), generator=g) * 0.1 # bias for the second layer, initialized to zero at the beginning\n",
    "\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0 # batch normalization gain\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1 # batch normalization bias\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print('Number of parameters:', sum(p.numel() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # for convenience\n",
    "# construct a mini-batch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) # random indices\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # the mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4257, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, chunkated into smaller steps so its easy to perform backward pass\n",
    "emb = C[Xb] # embedding \n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the embeddings\n",
    "# Linear layer\n",
    "hprebn = embcat @ W1 + b1 # linear transformation\n",
    "\n",
    "# Batch normalization\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True) # mean\n",
    "bndiff = hprebn - bnmeani # difference\n",
    "bndiff2 = bndiff ** 2 # squared difference\n",
    "bnvar = 1/(n-1) * (bndiff2).sum(0, keepdim=True) # variance, bessels correction, n-1 and not n\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5 # inverse square root, with epsilon for numerical stability\n",
    "bnraw = bndiff * bnvar_inv # normalized input\n",
    "hpreact = bngain * bnraw + bnbias # scale and shift\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # non-linearity\n",
    "\n",
    "# Second linear layer\n",
    "logits = h @ W2 + b2 # logits\n",
    "\n",
    "# Loss\n",
    "# same as F.cross_entropy(logits, Yb) # cross-entropy loss\n",
    "logit_maxes = logits.max(1, keepdim=True).values # max value of each row\n",
    "norm_logits = logits - logit_maxes # subtract the max value for numerical stability\n",
    "counts = norm_logits.exp() # exponentiate\n",
    "counts_sum = counts.sum(1, keepdim=True) # sum of exponentiated values\n",
    "counts_sum_inv = counts_sum ** -1 # inverse\n",
    "probs = counts * counts_sum_inv # probabilities\n",
    "logprobs = probs.log() # log-probabilities\n",
    "loss = -logprobs[range(n), Yb].mean() # cross-entropy loss\n",
    "\n",
    "# backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, bnmeani, hprebn, embcat, emb]:\n",
    "    t.retain_grad() # we need to retain the gradients for these tensors\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual computation of gradients\n",
    "############################ logprobs ############################\n",
    "logprobs.shape # (32, 27)\n",
    "Yb # index of the correct character\n",
    "logprobs[range(n), Yb] # indices of the correct characters\n",
    "logprobs[range(n), Yb].mean() # loss\n",
    "\n",
    "# loss = -(a + b + c)/3\n",
    "# loss = -1/3a - 1/3b - 1/3c\n",
    "# dloss/da = -1/3\n",
    "# dloss/db = -1/3\n",
    "# dloss/dc = -1/3\n",
    "dloss = -1/n # gradient of the loss with respect to the average of the log-probabilities\n",
    "# remember that the derivative of loss w.r.t to numbers which do not get pluked out of large array doesnt matter, becuase they do not contribute to the loss\n",
    "\n",
    "############################ probs ############################\n",
    "# dloss/dprobs = dlogprobs * dprobs # chain rule\n",
    "# dprobs = 1/probs\n",
    "\n",
    "############################ counts_sum_inv & counts ############################\n",
    "# dloss/dcounts_sum_inv = dprobs * dcounts_sum_inv\n",
    "# dcounts_sum_inv = counts\n",
    "counts.shape, counts_sum_inv.shape # (32, 27), (32, 1), broadcasting will be used\n",
    "# c = a * b, but with tensors\n",
    "# a[3*3], b[3*1] -->\n",
    "# a11 * b11, a12 * b11, a13 * b11\n",
    "# a21 * b11, a22 * b11, a23 * b11\n",
    "# a31 * b11, a32 * b11, a33 * b11\n",
    "# c[3*3]\n",
    "# two operations happen here, replication during broadcasting and then multiplication\n",
    "# remember that b11 is replicated 3 times, and then multiplied with a11, a12, a13, so we take sum the derivatives of c w.r.t to a11, a12, a13\n",
    "\n",
    "# dloss/dcounts = dprobs * dcounts\n",
    "# dcounts = counts_sum_inv\n",
    "# remeber not to cmp dcounts, since this is just the first contribution of counts, there will be another contribution\n",
    "\n",
    "############################ counts_sum ############################\n",
    "# dloss/dcounts_sum = dcounts_sum_inv * dcounts_sum\n",
    "# dcounts_sum = -1/counts_sum^2\n",
    "\n",
    "############################ counts ############################\n",
    "# dloss/dcounts = dcounts_sum * dcounts\n",
    "# dcounts = \n",
    "counts.shape, counts_sum.shape # (32, 27), (32, 1), broadcasting will be used\n",
    "# a11 a12 a13 ---> b1 (= a11 + a12 + a13)\n",
    "# a21 a22 a23 ---> b2 (= a21 + a22 + a23)\n",
    "# a31 a32 a33 ---> b3 (= a31 + a32 + a33)\n",
    "\n",
    "############################ norm_logits & logits ############################\n",
    "# dloss/dnorm_logits = dcounts * dnorm_logits\n",
    "# dnorm_logits = 1\n",
    "\n",
    "############################ norm_logits ############################\n",
    "norm_logits.shape, logits.shape, logit_maxes.shape # (32, 27), (32, 1), broadcasting will be used\n",
    "\n",
    "# c11 c12 c13 ---> a11 a12 a13    b1\n",
    "# c21 c22 c23 ---> a21 a22 a23 -  b2\n",
    "# c31 c32 c33 ---> a31 a32 a33    b3\n",
    "\n",
    "# c32 = a32 - b3\n",
    "# derivatives of c w.r.t a will be 1's and w.r.t b will be -1's. Therfore, c flows as it is, and a flows as it is, but b flows with a negative sign (sum for )\n",
    "\n",
    "############################ logits ############################\n",
    "# dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes # gradient of the loss with respect to the logitsx\n",
    "\n",
    "# see how the max of the logits is displayed\n",
    "# plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17905a920>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff=0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff=0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff=0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff=0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff=0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff=0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff=0.0\n",
      "dlogits         | exact: True  | approximate: True  | maxdiff=0.0\n"
     ]
    }
   ],
   "source": [
    "# Lets calculate the gradients manually\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n # gradient of the loss with respect to the log-probabilities\n",
    "\n",
    "dprobs = (1.0/probs) * dlogprobs # chain rule\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # sum of the gradients of the loss with respect to the sum of the exponentiated values\n",
    "dcounts = counts_sum_inv * dprobs # gradient of the loss with respect to the exponentiated values, broadcasting will be used here\n",
    "# remeber not to cmp dcounts, since this is just the first contribution of counts, there will be another contribution\n",
    "\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv # gradient of the loss with respect to the sum of the exponentiated values\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "dnorm_logits = torch.exp(norm_logits) * dcounts # gradient of the loss with respect to the normalized logits\n",
    "\n",
    "# derivatives of c w.r.t a will be 1's and w.r.t b will be -1's. Therfore, c flows as it is, and a flows as it is, but b flows with a negative sign (sum for )\n",
    "dlogits = dnorm_logits.clone() # gradient of the loss with respect to the logits\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) # gradient of the loss with respect to the max values\n",
    "\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes # gradient of the loss with respect to the logitsx\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('dlogits', dlogits, logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
